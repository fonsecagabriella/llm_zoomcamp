{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe484e57",
   "metadata": {},
   "source": [
    "# Agents + RAG - Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e99bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minsearch\n",
      "  Downloading minsearch-0.0.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from minsearch) (2.1.0)\n",
      "Requirement already satisfied: pandas in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from minsearch) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from minsearch) (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pandas->minsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pandas->minsearch) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pandas->minsearch) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->minsearch) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from scikit-learn->minsearch) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from scikit-learn->minsearch) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from scikit-learn->minsearch) (3.5.0)\n",
      "Downloading minsearch-0.0.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: minsearch\n",
      "Successfully installed minsearch-0.0.4\n"
     ]
    }
   ],
   "source": [
    "# install minsearch\n",
    "!pip install minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7493e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the documents\n",
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7d5288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x103ce9000>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index the documents\n",
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64cdd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now search for a question\n",
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac1fc1",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- This search function is the foundation of our RAG system.\n",
    "- It looks up in the FAQ to find relevant information.\n",
    "- The result is used to build context for the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c833cf",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "We create a function to format the search results into a structured context that our LLM can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167424c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b2f7e",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Takes search results\n",
    "- Formats each document\n",
    "- Put everything in a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e38434",
   "metadata": {},
   "source": [
    "## The RAG flow\n",
    "\n",
    "We add a call to an LLM and combine everything into a complete RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446e1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be7fe4",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- `build_prompt`: Formats the search results into a prompt\n",
    "- `llm`: Makes the API call to the language model\n",
    "- `rag`: Combines search and LLM into a single function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789c6e8",
   "metadata": {},
   "source": [
    "## Part 1: Agentic RAG\n",
    "Now let's make our flow agentic\n",
    "\n",
    "### Agents and Agentic flows\n",
    "\n",
    "**Agents are AI systems that can:**\n",
    "\n",
    "- Make decisions about what actions to take\n",
    "- Use tools to accomplish tasks\n",
    "- Maintain state and context\n",
    "- Learn from previous interactions\n",
    "- Work towards specific goals\n",
    "\n",
    "Agentic flow is not necessarily a completely independent agent, but it can still make some decisions during the flow execution\n",
    "\n",
    "**A typical agentic flow consists of:**\n",
    "\n",
    "1. Receiving a user request\n",
    "2. Analyzing the request and available tools\n",
    "3. Deciding on the next action\n",
    "4. Executing the action using appropriate tools\n",
    "5. Evaluating the results\n",
    "6. Either completing the task or continuing with more actions\n",
    "\n",
    "**The key difference from basic RAG is that agents can:**\n",
    "\n",
    "- Make multiple search queries\n",
    "- Combine information from different sources\n",
    "- Decide when to stop searching\n",
    "- Use their own knowledge when appropriate\n",
    "- Chain multiple actions together\n",
    "\n",
    "So in **agentic RAG**, the system\n",
    "- has access to the **history of previous actions**\n",
    "- **makes decisions independently** based on the current information and the previous actions\n",
    "\n",
    "Let's implement this step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49541f",
   "metadata": {},
   "source": [
    "### Making RAG more agentic\n",
    "\n",
    "First, we'll take the prompt we have so far and make it a little more \"agentic\":\n",
    "\n",
    "- Tell the LLM that it can answer the question directly or look up context\n",
    "- Provide output templates\n",
    "- Show clearly what's the source of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c56526df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "At the beginning the context is EMPTY.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "If CONTEXT is EMPTY, you can use our FAQ database.\n",
    "In this case, use the following output template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\"\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dfcc42",
   "metadata": {},
   "source": [
    "👩🏽‍💻 Let's use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7921ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "how do I run docker on gentoo?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "EMPTY\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To run Docker on Gentoo, you'll first need to install Docker. You can do this by emerging the Docker package. Use the following command:\\n\\n```bash\\nemerge app-emulation/docker\\n```\\n\\nAfter the installation, you will need to start the Docker service. You can do this with:\\n\\n```bash\\nrc-service docker start\\n```\\n\\nTo have Docker start automatically on boot, you can add it to the default runlevel with:\\n\\n```bash\\nrc-update add docker default\\n```\\n\\nOnce Docker is running, you can test it by running:\\n\\n```bash\\ndocker run hello-world\\n```\\n\\nThis will download the 'hello-world' image and run it, confirming your Docker installation is working correctly.\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I run docker on gentoo?\"\n",
    "context = \"EMPTY\"\n",
    "\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)\n",
    "\n",
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e993ca1",
   "metadata": {},
   "source": [
    "👀 Implementing **MAKE THE SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8151e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4406fc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "how do I run docker on gentoo?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - Error response from daemon: error while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists\n",
      "answer: When you run this command second time\n",
      "docker run -it \\\n",
      "-e POSTGRES_USER=\"root\" \\\n",
      "-e POSTGRES_PASSWORD=\"root\" \\\n",
      "-e POSTGRES_DB=\"ny_taxi\" \\\n",
      "-v <your path>:/var/lib/postgresql/data \\\n",
      "-p 5432:5432 \\\n",
      "postgres:13\n",
      "The error message above could happen. That means you should not mount on the second run. This command helped me:\n",
      "When you run this command second time\n",
      "docker run -it \\\n",
      "-e POSTGRES_USER=\"root\" \\\n",
      "-e POSTGRES_PASSWORD=\"root\" \\\n",
      "-e POSTGRES_DB=\"ny_taxi\" \\\n",
      "-p 5432:5432 \\\n",
      "postgres:13\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - The input device is not a TTY (Docker run for Windows)\n",
      "answer: You may have this error:\n",
      "$ docker run -it ubuntu bash\n",
      "the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\n",
      "error:\n",
      "Solution:\n",
      "Use winpty before docker command (source)\n",
      "$ winpty docker run -it ubuntu bash\n",
      "You also can make an alias:\n",
      "echo \"alias docker='winpty docker'\" >> ~/.bashrc\n",
      "OR\n",
      "echo \"alias docker='winpty docker'\" >> ~/.bash_profile\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" : open //./pipe/docker_engine: The system cannot find the file specified\n",
      "answer: As the official Docker for Windows documentation says, the Docker engine can either use the\n",
      "Hyper-V or WSL2 as its backend. However, a few constraints might apply\n",
      "Windows 10 Pro / 11 Pro Users: \n",
      "In order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\n",
      "Windows 10 Home / 11 Home Users: \n",
      "On the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\n",
      "You can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\n",
      "In case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \n",
      "\n",
      "https://github.com/microsoft/WSL/issues/5393\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?\n",
      "answer: It is recommended by the Docker do\n",
      "[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\n",
      "If even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied\n",
      "answer: This happens if you did not create the docker group and added your user. Follow these steps from the link:\n",
      "guides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\n",
      "And then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\n",
      "If you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\n",
      "In your docker-compose.yaml file, enter the following into your pgAdmin declaration:\n",
      "volumes:\n",
      "- type: volume\n",
      "source: pgadmin_data\n",
      "target: /var/lib/pgadmin\n",
      "Also add the following to the end of the file:ls\n",
      "volumes:\n",
      "Pgadmin_data:\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "search_results = search(question)\n",
    "context = build_context(search_results)\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52b7a778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To run Docker on Gentoo, you will need to follow these steps: \\n\\n1. **Install Docker:** You can install Docker using the Portage package manager. Run the following command: \\n   ```bash\\n   sudo emerge app-emulation/docker\\n   ```\\n\\n2. **Start the Docker service:** You need to start the Docker daemon for it to function properly. You can do this using OpenRC (the default init system for Gentoo) by running: \\n   ```bash\\n   sudo rc-service docker start\\n   ```\\n\\n3. **Add your user to the docker group:** This will allow you to run Docker commands without using 'sudo'. Run: \\n   ```bash\\n   sudo gpasswd -a $USER docker\\n   ``` \\n   After adding your user, make sure to log out and log back in for the group changes to take effect.\\n\\n4. **Verify installation:** You can verify that Docker is running correctly by executing: \\n   ```bash\\n   docker run hello-world\\n   ``` \\n   This command should pull the hello-world image and run it, displaying a message if everything is set up correctly.\\n\\nEnsure your system has all necessary kernel features enabled, as Docker depends on specific kernel modules to function properly.\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# direct querying it\n",
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381b554",
   "metadata": {},
   "source": [
    "Let's put this together:\n",
    "\n",
    "- First attempt to answer it with our know knowledge\n",
    "- If needed, do the lookup and then answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b1bd194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def agentic_rag_v1(question):\n",
    "    context = \"EMPTY\"\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(answer)\n",
    "\n",
    "    if answer['action'] == 'SEARCH':\n",
    "        print('need to perform search...')\n",
    "        search_results = search(question)\n",
    "        context = build_context(search_results)\n",
    "        \n",
    "        prompt = prompt_template.format(question=question, context=context)\n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d61b952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'SEARCH', 'reasoning': 'The question about how to join the course requires checking the FAQ database as there is no information in the current context.'}\n",
      "need to perform search...\n",
      "{'action': 'ANSWER', 'answer': \"To join the course, make sure to register before it starts using the provided registration link. The course will commence on January 15, 2024, at 17:00. Additionally, you should join the course's public Google Calendar, Telegram channel for announcements, and Slack for communication. Even if you miss the registration deadline, you can still participate by submitting homework assignments, but be aware of the final project deadlines.\", 'source': 'CONTEXT'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To join the course, make sure to register before it starts using the provided registration link. The course will commence on January 15, 2024, at 17:00. Additionally, you should join the course's public Google Calendar, Telegram channel for announcements, and Slack for communication. Even if you miss the registration deadline, you can still participate by submitting homework assignments, but be aware of the final project deadlines.\",\n",
       " 'source': 'CONTEXT'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it\n",
    "agentic_rag_v1('how do I join the course?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdcbf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER', 'answer': \"In 'Gossip Girl,' a drama series set in New York City, the story revolves around the lives of privileged teenagers attending an elite private school on the Upper East Side. The plot is narrated by an anonymous blogger known as 'Gossip Girl,' who exposes the secrets and scandalous affairs of the characters. Central themes include friendship, love, betrayal, and the complexities of adolescence as the characters navigate their social lives, relationships, and personal challenges. The show features a mix of romance, intrigue, and social commentary, culminating in various twists and turns throughout its seasons.\", 'source': 'OWN_KNOWLEDGE'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"In 'Gossip Girl,' a drama series set in New York City, the story revolves around the lives of privileged teenagers attending an elite private school on the Upper East Side. The plot is narrated by an anonymous blogger known as 'Gossip Girl,' who exposes the secrets and scandalous affairs of the characters. Central themes include friendship, love, betrayal, and the complexities of adolescence as the characters navigate their social lives, relationships, and personal challenges. The show features a mix of romance, intrigue, and social commentary, culminating in various twists and turns throughout its seasons.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it again\n",
    "agentic_rag_v1('what happens in gossip girl?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81e38a",
   "metadata": {},
   "source": [
    "## Part 2: Agentic search\n",
    "So far we had two actions only: search and answer.\n",
    "\n",
    "But we can let our \"agent\" formulate one or more search queries - and do it for a few iterations until we found an answer\n",
    "\n",
    "Let's build a prompt:\n",
    "\n",
    "1. List available actions:\n",
    "    - Search in FAQ\n",
    "    - Answer using own knowledge\n",
    "    - Answer using information extracted from FAQ\n",
    "2. Provide access to the previous actions\n",
    "3. Have clear stop criteria (no more than X iterations)\n",
    "4. We also specify the output format, so it's easier to parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a58da1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "\n",
    "The CONTEXT is build with the documents from our FAQ database.\n",
    "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
    "from FAQ to and add them to the context.\n",
    "PREVIOUS_ACTIONS contains the actions you already performed.\n",
    "\n",
    "At the beginning the CONTEXT is empty.\n",
    "\n",
    "You can perform the following actions:\n",
    "\n",
    "- Search in the FAQ database to get more data for the CONTEXT\n",
    "- Answer the question using the CONTEXT\n",
    "- Answer the question using your own knowledge\n",
    "\n",
    "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
    "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
    "\n",
    "Don't use search queries used at the previous iterations.\n",
    "\n",
    "Don't repeat previously performed actions.\n",
    "\n",
    "Don't perform more than {max_iterations} iterations for a given student question.\n",
    "The current iteration number: {iteration_number}. If we exceed the allowed number \n",
    "of iterations, give the best possible answer with the provided information.\n",
    "\n",
    "Output templates:\n",
    "\n",
    "If you want to perform search, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\",\n",
    "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER_CONTEXT\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<SEARCH_QUERIES>\n",
    "{search_queries}\n",
    "</SEARCH_QUERIES>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "<PREVIOUS_ACTIONS>\n",
    "{previous_actions}\n",
    "</PREVIOUS_ACTIONS>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1fc5d3",
   "metadata": {},
   "source": [
    "Our code becomes more complicated. For the first iteration, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8c31690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I join the course?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=1\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ab17112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"The student's question pertains to joining the course, which likely involves specific enrollment procedures, eligibility, and other related details not currently available in the context. Therefore, I need to search for information related to course enrollment.\",\n",
      "  \"keywords\": [\n",
      "    \"how to enroll in the course\",\n",
      "    \"course registration process\",\n",
      "    \"joining the course details\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)\n",
    "print(json.dumps(answer, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2942bc",
   "metadata": {},
   "source": [
    "We need to sabe the actions, so let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ac932a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_actions.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b57ab",
   "metadata": {},
   "source": [
    "Save the search queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c44f8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = answer['keywords']\n",
    "search_queries.extend(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02e061",
   "metadata": {},
   "source": [
    "And perform the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a056dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keywords:\n",
    "    res = search(k)\n",
    "    search_results.extend(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7144bbb",
   "metadata": {},
   "source": [
    "Some of the search results will be duplicates, so we need to remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "990fd9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        _id = el['_id']\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        result.append(el)\n",
    "    return result\n",
    "\n",
    "search_results = dedup(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c54259",
   "metadata": {},
   "source": [
    "Now let's make another iteration - use the same code as previously, but remove variable initialisation and increase the iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59c0e2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "how to enroll in the course\n",
      "course registration process\n",
      "joining the course details\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: lsRuntimeError: Java gateway process exited before sending its port number\n",
      "answer: After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession\n",
      "spark = SparkSession.builder \\\n",
      ".master(\"local[*]\") \\\n",
      ".appName('test') \\\n",
      ".getOrCreate()\n",
      "df = spark.read \\\n",
      ".option(\"header\", \"true\") \\\n",
      ".csv('taxi+_zone_lookup.csv')\n",
      "df.show()\n",
      "it gives the error:\n",
      "RuntimeError: Java gateway process exited before sending its port number\n",
      "✅The solution (for me) was:\n",
      "pip install findspark on the command line and then\n",
      "Add\n",
      "import findspark\n",
      "findspark.init()\n",
      "to the top of the script.\n",
      "Another possible solution is:\n",
      "Check that pyspark is pointing to the correct location.\n",
      "Run pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\n",
      "If it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\n",
      "To add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\n",
      "Once everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The student's question pertains to joining the course, which likely involves specific enrollment procedures, eligibility, and other related details not currently available in the context. Therefore, I need to search for information related to course enrollment.\", \"keywords\": [\"how to enroll in the course\", \"course registration process\", \"joining the course details\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER_CONTEXT\",\n",
      "  \"answer\": \"To join the course, you need to register before the course starts using the provided link. You can also join the course's public Google Calendar for updates and announcements via the Telegram channel. It's noted that registration is not strictly necessary, so you can start learning and submitting homework without formal registration, as it's mainly to gauge interest before the start date.\",\n",
      "  \"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# question = \"how do I join the course?\"\n",
    "\n",
    "# search_queries = []\n",
    "# search_results = []\n",
    "# previous_actions = []\n",
    "\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=2\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)\n",
    "print(json.dumps(answer, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77672c",
   "metadata": {},
   "source": [
    "Let's put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "698681af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To provide a comprehensive answer about success strategies for module 1, I need to find relevant information that outlines best practices or tips specific to this module. This will help guide the student effectively.\",\n",
      "  \"keywords\": [\n",
      "    \"success in module 1\",\n",
      "    \"tips for module 1\",\n",
      "    \"module 1 best practices\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "tips for module 1\n",
      "module 1 best practices\n",
      "success in module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).\n",
      "answer: More info in the Docker Docs on Best Practises\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer about success strategies for module 1, I need to find relevant information that outlines best practices or tips specific to this module. This will help guide the student effectively.\", \"keywords\": [\"success in module 1\", \"tips for module 1\", \"module 1 best practices\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To further enhance the context for the student's query, I will look for specific resources or documents that provide guidance on being successful in Module 1, focusing on Docker and Terraform, as well as any tips or best practices that could help the student excel in this module.\",\n",
      "  \"keywords\": [\n",
      "    \"Module 1 Docker best practices\",\n",
      "    \"Module 1 Terraform tips\",\n",
      "    \"success strategies in Docker and Terraform\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "Module 1 Terraform tips\n",
      "success strategies in Docker and Terraform\n",
      "module 1 best practices\n",
      "Module 1 Docker best practices\n",
      "tips for module 1\n",
      "success in module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).\n",
      "answer: More info in the Docker Docs on Best Practises\n",
      "\n",
      "section: Project\n",
      "question: Spark docker - `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You can get the version of py4j from inside docker using this command\n",
      "docker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error acquiring the state lock\n",
      "answer: https://github.com/hashicorp/terraform/issues/14513\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL.\n",
      "answer: When running\n",
      "terraform apply\n",
      "on wsl2 I've got this error:\n",
      "│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\n",
      "│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\n",
      "IT happens because there may be time desync on your machine which affects computing JWT\n",
      "To fix this, run the command\n",
      "sudo hwclock -s\n",
      "which fixes your system time.\n",
      "Reference\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g\n",
      "answer: You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?\n",
      "answer: Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’\n",
      "answer: The error:\n",
      "Error: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\n",
      "The solution:\n",
      "You have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later\n",
      "answer: It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer about success strategies for module 1, I need to find relevant information that outlines best practices or tips specific to this module. This will help guide the student effectively.\", \"keywords\": [\"success in module 1\", \"tips for module 1\", \"module 1 best practices\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To further enhance the context for the student's query, I will look for specific resources or documents that provide guidance on being successful in Module 1, focusing on Docker and Terraform, as well as any tips or best practices that could help the student excel in this module.\", \"keywords\": [\"Module 1 Docker best practices\", \"Module 1 Terraform tips\", \"success strategies in Docker and Terraform\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To be successful in Module 1, which focuses on Docker and Terraform, consider the following strategies: 1. **Understand the Fundamentals**: Make sure you have a solid grasp of Docker's and Terraform's core concepts, including containers, images, orchestration, and infrastructure as code. 2. **Hands-On Practice**: Engage in practical exercises; hands-on experience is crucial. Create small projects to apply what you learn in real scenarios. 3. **Follow Best Practices**: Use best practices in Docker for container management and in Terraform for infrastructure provisioning. For Docker, this includes optimizing your Dockerfile and keeping your images lightweight. For Terraform, manage your state files and structure your configuration files effectively. 4. **Documentation and Resources**: Regularly refer to the official documentation for both tools to deepen your understanding and keep updated with any changes. 5. **Engage with the Community**: Take part in forums or study groups to discuss topics and troubleshoot issues you encounter. Collaborating with peers can significantly enhance your learning experience.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"what do I need to do to be successful at module 1?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    print(f'ITERATION #{iteration}...')\n",
    "\n",
    "    context = build_context(search_results)\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        search_queries=\"\\n\".join(search_queries),\n",
    "        previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "        max_iterations=3,\n",
    "        iteration_number=iteration\n",
    "    )\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(json.dumps(answer, indent=2))\n",
    "\n",
    "    previous_actions.append(answer)\n",
    "\n",
    "    action = answer['action']\n",
    "    if action != 'SEARCH':\n",
    "        break\n",
    "\n",
    "    keywords = answer['keywords']\n",
    "    search_queries = list(set(search_queries) | set(keywords))\n",
    "    \n",
    "    for k in keywords:\n",
    "        res = search(k)\n",
    "        search_results.extend(res)\n",
    "\n",
    "    search_results = dedup(search_results)\n",
    "    \n",
    "    iteration = iteration + 1\n",
    "    if iteration >= 4:\n",
    "        break\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c5e19",
   "metadata": {},
   "source": [
    "Or, as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05c202fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_search(question):\n",
    "    search_queries = []\n",
    "    search_results = []\n",
    "    previous_actions = []\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        print(f'ITERATION #{iteration}...')\n",
    "    \n",
    "        context = build_context(search_results)\n",
    "        prompt = prompt_template.format(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            search_queries=\"\\n\".join(search_queries),\n",
    "            previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "            max_iterations=3,\n",
    "            iteration_number=iteration\n",
    "        )\n",
    "    \n",
    "        print(prompt)\n",
    "    \n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(json.dumps(answer, indent=2))\n",
    "\n",
    "        previous_actions.append(answer)\n",
    "    \n",
    "        action = answer['action']\n",
    "        if action != 'SEARCH':\n",
    "            break\n",
    "    \n",
    "        keywords = answer['keywords']\n",
    "        search_queries = list(set(search_queries) | set(keywords))\n",
    "\n",
    "        for k in keywords:\n",
    "            res = search(k)\n",
    "            search_results.extend(res)\n",
    "    \n",
    "        search_results = dedup(search_results)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        if iteration >= 4:\n",
    "            break\n",
    "    \n",
    "        print()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e86b30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To provide a comprehensive answer on how to prepare for the course, I need to gather specific guidelines or suggestions that may be available in the FAQ database.\",\n",
      "  \"keywords\": [\n",
      "    \"course preparation\",\n",
      "    \"how to prepare for the course\",\n",
      "    \"study tips for the course\",\n",
      "    \"initial steps for course readiness\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "course preparation\n",
      "study tips for the course\n",
      "initial steps for course readiness\n",
      "how to prepare for the course\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Workshop 2 - RisingWave\n",
      "question: Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.\n",
      "answer: Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\n",
      "Use the git bash terminal in windows.\n",
      "Activate python venv from git bash: source .venv/Scripts/activate\n",
      "Modify the seed_kafka.py file: in the first line, replace python3 with python.\n",
      "Now from git bash, run the seed-kafka cmd. It should work now.\n",
      "Additional Notes:\n",
      "You can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\n",
      "The equivalent of source commands.sh  in Powershell is . .\\commands.sh from the workshop directory.\n",
      "Hope this can save you from some trouble in case you're doing this workshop on Windows like I am.\n",
      "—--------------------------------------------------------------------------------------\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer on how to prepare for the course, I need to gather specific guidelines or suggestions that may be available in the FAQ database.\", \"keywords\": [\"course preparation\", \"how to prepare for the course\", \"study tips for the course\", \"initial steps for course readiness\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"The CONTEXT does not contain specific tips or steps for preparing for the course. I need to find detailed preparation guidelines or study resources to assist the student effectively.\",\n",
      "  \"keywords\": [\n",
      "    \"course preparation tips\",\n",
      "    \"study tips\",\n",
      "    \"how to get ready for the course\",\n",
      "    \"initial steps for course preparation\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "course preparation tips\n",
      "how to prepare for the course\n",
      "how to get ready for the course\n",
      "initial steps for course readiness\n",
      "study tips\n",
      "initial steps for course preparation\n",
      "course preparation\n",
      "study tips for the course\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Workshop 2 - RisingWave\n",
      "question: Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.\n",
      "answer: Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\n",
      "Use the git bash terminal in windows.\n",
      "Activate python venv from git bash: source .venv/Scripts/activate\n",
      "Modify the seed_kafka.py file: in the first line, replace python3 with python.\n",
      "Now from git bash, run the seed-kafka cmd. It should work now.\n",
      "Additional Notes:\n",
      "You can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\n",
      "The equivalent of source commands.sh  in Powershell is . .\\commands.sh from the workshop directory.\n",
      "Hope this can save you from some trouble in case you're doing this workshop on Windows like I am.\n",
      "—--------------------------------------------------------------------------------------\n",
      "\n",
      "section: Module 3: Data Warehousing\n",
      "question: GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?\n",
      "answer: Check the Schema\n",
      "You might have a wrong formatting\n",
      "Try to upload the CSV.GZ files without formatting or going through pandas via wget\n",
      "See this Slack conversation for helpful tips\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "answer: If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\n",
      "For Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\n",
      "Additional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I get support if I take the course in the self-paced mode?\n",
      "answer: Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\n",
      "You can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer on how to prepare for the course, I need to gather specific guidelines or suggestions that may be available in the FAQ database.\", \"keywords\": [\"course preparation\", \"how to prepare for the course\", \"study tips for the course\", \"initial steps for course readiness\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The CONTEXT does not contain specific tips or steps for preparing for the course. I need to find detailed preparation guidelines or study resources to assist the student effectively.\", \"keywords\": [\"course preparation tips\", \"study tips\", \"how to get ready for the course\", \"initial steps for course preparation\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To prepare for the course effectively, consider the following steps: \\n\\n1. **Register Early**: Ensure you register for the course before it starts. This is important to gain access to materials and communications.\\n2. **Join Communication Channels**: Sign up for the course Telegram channel for announcements and join DataTalks.Club's Slack for community engagement and support.\\n3. **Study Resources**: Familiarize yourself with general course materials and review any recommended reading or resources provided by the instructors.\\n4. **Technical Setup**: Make sure your technical environment is ready, which may include downloading necessary software tools and setting up Git/GitHub as needed for course assignments.\\n5. **Time Management**: Plan a study schedule to keep up with the course pace when it begins.\\n6. **Pre-course Information**: Look for any pre-course materials that might be shared ahead of the start date, and read through all available documentation.\\n\\nThese steps should help you start on the right foot as you prepare for the course.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To prepare for the course effectively, consider the following steps: \\n\\n1. **Register Early**: Ensure you register for the course before it starts. This is important to gain access to materials and communications.\\n2. **Join Communication Channels**: Sign up for the course Telegram channel for announcements and join DataTalks.Club's Slack for community engagement and support.\\n3. **Study Resources**: Familiarize yourself with general course materials and review any recommended reading or resources provided by the instructors.\\n4. **Technical Setup**: Make sure your technical environment is ready, which may include downloading necessary software tools and setting up Git/GitHub as needed for course assignments.\\n5. **Time Management**: Plan a study schedule to keep up with the course pace when it begins.\\n6. **Pre-course Information**: Look for any pre-course materials that might be shared ahead of the start date, and read through all available documentation.\\n\\nThese steps should help you start on the right foot as you prepare for the course.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it\n",
    "agentic_search('how do I prepare for the course?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a1487d",
   "metadata": {},
   "source": [
    "## Part 3: Function Calling\n",
    "\n",
    "### **Function Calling in OpenAI**\n",
    "\n",
    "We put all this logic inside our prompt.\n",
    "\n",
    "But OpenAI and other providers provide a convenient API for adding extra functionality like search.\n",
    "\n",
    "[https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "\n",
    "It's called \"function calling\" - you define functions that the model can call, and if it decides to make a call, it returns structured output for that.\n",
    "\n",
    "For example, let's take our search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a693913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e2f2e",
   "metadata": {},
   "source": [
    "We describe it like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1ad292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fa4e7",
   "metadata": {},
   "source": [
    "Here we have:\n",
    "\n",
    "- `name`: `search`\n",
    "- `description`: when to use it\n",
    "- `parameters`: all the arguments that the function can take and their description\n",
    "\n",
    "In order to use function calling, we'll use a newer API - the \"responses\" API (not \"chat completions\" as previously):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8f1f580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"do well in module 1\"}', call_id='call_atn301uMuGbUlUilPpHNBXaF', name='search', type='function_call', id='fc_687fdadd907c81928231caf3c04fa4c703b79f1d22fc6ddd', status='completed')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I do well in module 1?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c47a3b",
   "metadata": {},
   "source": [
    "If the model thinkgs we should make a function call, it will tell us:\n",
    "\n",
    "`[ResponseFunctionToolCall(arguments='{\"query\":\"How to do well in module 1\"}', call_id='call_AwYwOak5Ljeidh4HbE3RxMZJ', name='search', type='function_call', id='fc_6848604db67881a298ec38121c1555ef0dee5fa0cdb59912', status='completed')]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f97f9",
   "metadata": {},
   "source": [
    "Let's make a call to `search`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48537ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'do well in module 1'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calls = response.output\n",
    "call = calls[0]\n",
    "call\n",
    "\n",
    "call_id = call.call_id\n",
    "call_id\n",
    "\n",
    "f_name = call.name\n",
    "f_name\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b3bbe",
   "metadata": {},
   "source": [
    "Using `f_name`we can find the function we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e25ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = globals()[f_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b19a2",
   "metadata": {},
   "source": [
    "And invoke it with the arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "264caaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = f(**arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de743f9",
   "metadata": {},
   "source": [
    "Now let's sabe the results as json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6939ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
      "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
      "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 299\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\\\"Murray Hill\\\") & (new_final.b_zone==\\\"Midwood\\\")).show()\\nKrishna Anand\",\n",
      "    \"section\": \"Module 5: pyspark\",\n",
      "    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 322\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
      "    \"section\": \"Module 5: pyspark\",\n",
      "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 323\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 112\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Error raised during the jupyter notebook\\u2019s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module \\u201cpsycopg2\\u201d. Can be installed by Conda or pip.\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 125\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "search_results = json.dumps(results, indent=2)\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9306a65",
   "metadata": {},
   "source": [
    "And save both the response and the result of the function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd7c4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages.append(call)\n",
    "\n",
    "chat_messages.append({\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": search_results,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8af7be",
   "metadata": {},
   "source": [
    "Now `chat_messages` contains both the call description (so it keeps track of history) and the results.\n",
    "\n",
    "Let's make another call to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37ec6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9699b6f",
   "metadata": {},
   "source": [
    "This time it should be the response (but also can be another call):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40fbf3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To do well in Module 1 of your course, here are some tips based on common challenges and strategies for success:\n",
      "\n",
      "1. **Understand the Basics**: Ensure you have a good grasp of the foundational concepts covered in Module 1. This includes Docker and Terraform, as well as how they relate to your course subject.\n",
      "\n",
      "2. **Installation and Setup**: Pay close attention to the installation steps. Common issues include module not found errors (such as 'psycopg2'). Make sure you have the necessary libraries installed. For example, you can install `psycopg2` using:\n",
      "   ```bash\n",
      "   pip install psycopg2-binary\n",
      "   ```\n",
      "\n",
      "3. **Hands-On Practice**: Engage actively with the practical components of the module. Create and manage Docker containers as instructed and practice writing Terraform configurations.\n",
      "\n",
      "4. **Check Dependencies**: If you encounter errors, verify that all required modules are installed and check that your environmental paths are set correctly.\n",
      "\n",
      "5. **Utilize Resources**: Refer to the course materials and any supplementary resources available. Forums, study groups, and TA support can be great for clarifying doubts.\n",
      "\n",
      "6. **Regular Review**: After completing each section, revisit the material to reinforce your learning. This can help solidify your understanding and prepare you for assessments.\n",
      "\n",
      "By following these strategies, you can enhance your performance in Module 1. If you have specific topics or questions about the module, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "r = response.output[0]\n",
    "print(r.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e116e",
   "metadata": {},
   "source": [
    "### **Making multiple calls**\n",
    "\n",
    "\n",
    "What if we want to make multiple calls? Change the developer prompt a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5832f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "If you look up something in FAQ, convert the student question into multiple queries.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dfb32d",
   "metadata": {},
   "source": [
    "This time let's start to organise the code a litte.\n",
    "\n",
    "First, create a function `do_call`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dea76faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_call(tool_call_response):\n",
    "    function_name = tool_call_response.name\n",
    "    arguments = json.loads(tool_call_response.arguments)\n",
    "\n",
    "    f = globals()[function_name]\n",
    "    result = f(**arguments)\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": tool_call_response.call_id,\n",
    "        \"output\": json.dumps(result, indent=2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7158c9",
   "metadata": {},
   "source": [
    "Now iterate over responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23da9b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call\n",
      "function_call\n",
      "function_call\n"
     ]
    }
   ],
   "source": [
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "\n",
    "    if entry.type == 'function_call':      \n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.type == 'message':\n",
    "        print(entry.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642fe61",
   "metadata": {},
   "source": [
    "First call will the probably be a `function call`, so let's do another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53208f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n",
      "\n",
      "To excel in Module 1, here are some useful tips and strategies:\n",
      "\n",
      "### 1. Understand the Key Topics\n",
      "Module 1 focuses on **Docker and Terraform**, which are essential tools in data engineering. Make sure you review the following:\n",
      "\n",
      "- **Docker Basics**: Understand containerization, images, and how to work with Docker containers.\n",
      "- **Terraform Fundamentals**: Familiarize yourself with infrastructure as code, creating and managing infrastructure using Terraform.\n",
      "\n",
      "### 2. Follow the Course Instructions\n",
      "Pay close attention to the course materials and instructions. Here are some common challenges and solutions from students:\n",
      "\n",
      "- **ModuleNotFoundError** for libraries like `psycopg2`:\n",
      "  - If you encounter `ModuleNotFoundError: No module named 'psycopg2'`, you may need to install it using:\n",
      "    ```bash\n",
      "    pip install psycopg2-binary\n",
      "    ```\n",
      "  - If you've already installed it, try updating it with:\n",
      "    ```bash\n",
      "    pip install psycopg2-binary --upgrade\n",
      "    ```\n",
      "\n",
      "- **SQLAlchemy Errors**: To resolve issues like `TypeError: 'module' object is not callable`, ensure that your connection string is correctly formatted:\n",
      "    ```python\n",
      "    conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "    ```\n",
      "\n",
      "### 3. Leverage Resources\n",
      "Check for additional resources provided in the course:\n",
      "\n",
      "- **Documentation**: The official Docker and Terraform documentation can provide deeper insights.\n",
      "- **Practice**: Try to set up your Docker containers and Terraform configurations in a local environment. This practical experience is invaluable.\n",
      "\n",
      "### 4. Troubleshooting & Community\n",
      "If you encounter issues:\n",
      "\n",
      "- **Forums and Community**: Engage with fellow students and forums for shared experiences and troubleshooting tips.\n",
      "- **Regular Updates**: Ensure all your software (like WSL, Docker, etc.) is up-to-date, as outdated software can lead to issues.\n",
      "\n",
      "### 5. Manage Your Time\n",
      "Set a schedule that allows you to review content, complete hands-on labs, and participate in discussions. Consistent practice will help reinforce your learning.\n",
      "\n",
      "### Conclusion\n",
      "By following these strategies and utilizing the available resources effectively, you should be well-prepared to excel in Module 1. If you have specific questions or need clarification on certain topics, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "    print()\n",
    "\n",
    "    if entry.type == 'function_call':      \n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.type == 'message':\n",
    "        print(entry.content[0].text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc704234",
   "metadata": {},
   "source": [
    "### **Putting everthing together**\n",
    "\n",
    "But what if it's not?\n",
    "\n",
    "Let's make two loops:\n",
    "\n",
    "- First is the main Q&A loop - ask question, get back the answer\n",
    "- Second is the request loop - send requests until there's a message reply from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bdb0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "When using FAQ, perform deep topic exploration: make one request to FAQ,\n",
    "and then based on the results, make more requests.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you didn't include your question. Could you please provide more details or ask anything specific you'd like assistance with?\n",
      "\n",
      "It looks like your message didn't come through. Please try sending your question again, and I'll be happy to help!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True: # main Q&A loop\n",
    "    question = input() # How do I do my best for module 1?\n",
    "    if question == 'stop':\n",
    "        break\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True: # request-response loop - query API till get a message\n",
    "        response = client.responses.create(\n",
    "            model='gpt-4o-mini',\n",
    "            input=chat_messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        has_messages = False\n",
    "        \n",
    "        for entry in response.output:\n",
    "            chat_messages.append(entry)\n",
    "        \n",
    "            if entry.type == 'function_call':      \n",
    "                print('function_call:', entry)\n",
    "                print()\n",
    "                result = do_call(entry)\n",
    "                chat_messages.append(result)\n",
    "            elif entry.type == 'message':\n",
    "                print(entry.content[0].text)\n",
    "                print()\n",
    "                has_messages = True\n",
    "\n",
    "        if has_messages:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea099815",
   "metadata": {},
   "source": [
    "It's also possible that there's both message and tool calls, but we'll ignore this case for now. (It's easy to fix - just check if there are no function calls, and only then ask the user for input.)\n",
    "\n",
    "Let's make it a bit nicer using HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b810511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdown\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: markdown\n",
      "Successfully installed markdown-3.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9bfe23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Chat loop\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mquestion\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat ended.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'question' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import markdown # pip install markdown\n",
    "\n",
    "    \n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "]\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    \n",
    "    if question.strip().lower() == 'stop':\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "    print()\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True:  # inner request loop\n",
    "        response = client.responses.create(\n",
    "            model='gpt-4o-mini',\n",
    "            input=chat_messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        has_messages = False\n",
    "\n",
    "        for entry in response.output:\n",
    "            chat_messages.append(entry)\n",
    "\n",
    "            if entry.type == \"function_call\":\n",
    "                result = do_call(entry)\n",
    "                chat_messages.append(result)\n",
    "                display_function_call(entry, result)\n",
    "\n",
    "            elif entry.type == \"message\":\n",
    "                display_response(entry)\n",
    "                has_messages = True\n",
    "\n",
    "        if has_messages:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c00ff1",
   "metadata": {},
   "source": [
    "### **Using multiple tools**\n",
    "\n",
    "What if we also want to use this chat app to add new entries to the FAQ? We will need another function for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(question, answer):\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# description\n",
    "\n",
    "add_entry_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"add_entry\",\n",
    "    \"description\": \"Add an entry to the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question to be added to the FAQ database\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The answer to the question\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa1f0d",
   "metadata": {},
   "source": [
    "We can just reuse the preivous code. But we can also clean it up\n",
    "and make it more modular. \n",
    "\n",
    "See the result in [`chat_assistant.py`](chat_assistant.py)\n",
    "\n",
    "You can download it using `wget`:\n",
    "\n",
    "```bash\n",
    "wget https://raw.githubusercontent.com/alexeygrigorev/rag-agents-workshop/refs/heads/main/chat_assistant.py\n",
    "```\n",
    "\n",
    "Here we define multiple classes:\n",
    "\n",
    "- `Tools` - manages function tools for the agent\n",
    "    - `add_tool(function, description)`: Register a function with its description\n",
    "    - `get_tools()`: Return list of registered tool descriptions\n",
    "    - `function_call(tool_call_response)`: Execute a function call and return result\n",
    "- `ChatInterface` - handles user input and display formatting\n",
    "    - `input()`: Get user input\n",
    "    - `display(message)`: Print a message\n",
    "    - `display_function_call(entry, result)`: Show function calls in HTML format\n",
    "    - `display_response(entry)`: Display AI responses with markdown\n",
    "- `ChatAssistant` - main orchestrator for chat conversations.\n",
    "    - `__init__(tools, developer_prompt, chat_interface, client)`: Initialize assistant\n",
    "    - `gpt(chat_messages)`: Make OpenAI API calls\n",
    "    - `run()`: Main chat loop handling user input and AI responses\n",
    "\n",
    "Let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281601d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://raw.githubusercontent.com/alexeygrigorev/rag-agents-workshop/refs/heads/main/chat_assistant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chat_assistant\n",
    "\n",
    "tools = chat_assistant.Tools()\n",
    "tools.add_tool(search, search_tool)\n",
    "\n",
    "tools.get_tools()\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_interface = chat_assistant.ChatInterface()\n",
    "\n",
    "chat = chat_assistant.ChatAssistant(\n",
    "    tools=tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and run it\n",
    "chat.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's add the new tool\n",
    "tools.add_tool(add_entry, add_entry_description)\n",
    "tools.get_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66c194",
   "metadata": {},
   "source": [
    "And talk with the assistant:\n",
    "\n",
    "- How do I do well for module 1?\n",
    "- Add this back to FAQ\n",
    "\n",
    "And check that it's in the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.docs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f841d81",
   "metadata": {},
   "source": [
    "## Part 4: Using Pydantic AI\n",
    "\n",
    "### Installing and using PydanticAI\n",
    "\n",
    "There are frameworks that make it easier for us to create agents\n",
    "\n",
    "One of them is PydanticAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070b746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic-ai\n",
      "  Downloading pydantic_ai-0.4.5-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-ai-slim==0.4.5 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading pydantic_ai_slim-0.4.5-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.2.0)\n",
      "Collecting griffe>=1.3.2 (from pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.28.1)\n",
      "Collecting opentelemetry-api>=1.28.0 (from pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pydantic-graph==0.4.5 (from pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading pydantic_graph-0.4.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (2.10.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.4.1)\n",
      "Collecting groq>=0.19.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting ag-ui-protocol>=0.1.8 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading ag_ui_protocol-0.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: starlette>=0.45.3 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.45.3)\n",
      "Collecting mistralai>=1.2.5 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading mistralai-1.9.2-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting openai>=1.92.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting huggingface-hub>=0.33.2 (from huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting argcomplete>=3.5.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (3.0.43)\n",
      "Requirement already satisfied: rich>=13 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (14.0.0)\n",
      "Collecting google-genai>=1.24.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading google_genai-1.26.0-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: boto3>=1.37.24 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.38.46)\n",
      "Collecting anthropic>=0.52.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading anthropic-0.58.2-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting cohere>=5.13.11 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading cohere-5.16.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting mcp>=1.9.4 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading mcp-1.12.1-py3-none-any.whl.metadata (60 kB)\n",
      "Collecting pydantic-evals==0.4.5 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading pydantic_evals-0.4.5-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting google-auth>=2.36.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (2.32.3)\n",
      "Requirement already satisfied: anyio>=0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-evals==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (4.7.0)\n",
      "Collecting logfire-api>=1.2.0 (from pydantic-evals==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading logfire_api-4.0.0-py3-none-any.whl.metadata (971 bytes)\n",
      "Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-evals==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (6.0.2)\n",
      "Collecting pydantic>=2.10 (from pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.6.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.10->pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (4.12.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from anyio>=0->pydantic-evals==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (3.7)\n",
      "Requirement already satisfied: certifi in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from httpx>=0.27->pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from httpx>=0.27->pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.14.0)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.46 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.38.46)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from botocore<1.39.0,>=1.38.46->boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from botocore<1.39.0,>=1.38.46->boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.46->boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.17.0)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.13.11->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading fastavro-1.11.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere>=5.13.11->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from cohere>=5.13.11->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.21.1)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.13.11->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (3.3.2)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from huggingface-hub>=0.33.2->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from huggingface-hub>=0.33.2->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from huggingface-hub>=0.33.2->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from huggingface-hub>=0.33.2->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from huggingface-hub>=0.33.2->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.1.3)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.39.0,>=1.38.46->boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting anyio>=0 (from pydantic-evals==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting tenacity<9.0.0,>=8.2.3 (from google-genai>=1.24.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai>=1.24.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached websockets-15.0.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting colorama>=0.4 (from griffe>=1.3.2->pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (3.12.14)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (4.23.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.0.20)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading sse_starlette-2.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting uvicorn>=0.23.1 (from mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.22.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==0.4.5->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (3.23.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from prompt-toolkit>=3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.2.5)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from pydantic-settings>=2.5.2->mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.1.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from uvicorn>=0.23.1->mcp>=1.9.4->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (8.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.2; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.5->pydantic-ai) (1.18.0)\n",
      "Downloading pydantic_ai-0.4.5-py3-none-any.whl (10 kB)\n",
      "Downloading pydantic_ai_slim-0.4.5-py3-none-any.whl (247 kB)\n",
      "Downloading pydantic_evals-0.4.5-py3-none-any.whl (52 kB)\n",
      "Downloading pydantic_graph-0.4.5-py3-none-any.whl (27 kB)\n",
      "Downloading ag_ui_protocol-0.1.8-py3-none-any.whl (7.1 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "Downloading anthropic-0.58.2-py3-none-any.whl (292 kB)\n",
      "Downloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
      "Downloading cohere-5.16.1-py3-none-any.whl (291 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.11.1-cp310-cp310-macosx_10_9_universal2.whl (944 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m944.2/944.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading google_genai-1.26.0-py3-none-any.whl (217 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached websockets-15.0.1-cp310-cp310-macosx_10_9_x86_64.whl (173 kB)\n",
      "Using cached griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading groq-0.30.0-py3-none-any.whl (131 kB)\n",
      "Downloading logfire_api-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mcp-1.12.1-py3-none-any.whl (158 kB)\n",
      "Downloading mistralai-1.9.2-py3-none-any.whl (411 kB)\n",
      "Downloading openai-1.97.1-py3-none-any.whl (764 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.4/764.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading sse_starlette-2.4.1-py3-none-any.whl (10 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Installing collected packages: websockets, uvicorn, urllib3, tenacity, pydantic-core, pyasn1, logfire-api, httpx-sse, fastavro, eval-type-backport, colorama, cachetools, argcomplete, anyio, types-requests, sse-starlette, rsa, pydantic, pyasn1-modules, opentelemetry-api, griffe, pydantic-graph, openai, mistralai, huggingface-hub, groq, google-auth, anthropic, ag-ui-protocol, pydantic-ai-slim, mcp, google-genai, pydantic-evals, cohere, pydantic-ai\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 1.26.19\n",
      "\u001b[2K    Uninstalling urllib3-1.26.19:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled urllib3-1.26.19━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K  Attempting uninstall: tenacity━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: tenacity 9.1.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling tenacity-9.1.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled tenacity-9.1.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K  Attempting uninstall: pydantic-core━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.27.2━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling pydantic_core-2.27.2:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.27.2━━━━━━━━━━━\u001b[0m \u001b[32m 2/35\u001b[0m [urllib3]\n",
      "\u001b[2K  Attempting uninstall: anyio0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/35\u001b[0m [fastavro]core]\n",
      "\u001b[2K    Found existing installation: anyio 4.7.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/35\u001b[0m [fastavro]\n",
      "\u001b[2K    Uninstalling anyio-4.7.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/35\u001b[0m [fastavro]\n",
      "\u001b[2K      Successfully uninstalled anyio-4.7.0━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/35\u001b[0m [fastavro]\n",
      "\u001b[2K  Attempting uninstall: pydantic\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/35\u001b[0m [anyio]\n",
      "\u001b[2K    Found existing installation: pydantic 2.10.5━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/35\u001b[0m [anyio]\n",
      "\u001b[2K    Uninstalling pydantic-2.10.5:[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/35\u001b[0m [anyio]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.10.5━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/35\u001b[0m [anyio]\n",
      "\u001b[2K  Attempting uninstall: openai\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/35\u001b[0m [griffe]modules]\n",
      "\u001b[2K    Found existing installation: openai 1.77.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/35\u001b[0m [griffe]\n",
      "\u001b[2K    Uninstalling openai-1.77.0:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/35\u001b[0m [openai]\n",
      "\u001b[2K      Successfully uninstalled openai-1.77.0[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/35\u001b[0m [openai]\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m23/35\u001b[0m [mistralai]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.33.0━━━━━━━\u001b[0m \u001b[32m23/35\u001b[0m [mistralai]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.33.0:\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m23/35\u001b[0m [mistralai]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.33.0━━━━━━━━━\u001b[0m \u001b[32m23/35\u001b[0m [mistralai]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/35\u001b[0m [pydantic-ai]\u001b[0m [cohere]c-evals]m]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cognee 0.2.0 requires pydantic==2.10.5, but you have pydantic 2.11.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ag-ui-protocol-0.1.8 anthropic-0.58.2 anyio-4.9.0 argcomplete-3.6.2 cachetools-5.5.2 cohere-5.16.1 colorama-0.4.6 eval-type-backport-0.2.2 fastavro-1.11.1 google-auth-2.40.3 google-genai-1.26.0 griffe-1.7.3 groq-0.30.0 httpx-sse-0.4.0 huggingface-hub-0.33.4 logfire-api-4.0.0 mcp-1.12.1 mistralai-1.9.2 openai-1.97.1 opentelemetry-api-1.35.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.7 pydantic-ai-0.4.5 pydantic-ai-slim-0.4.5 pydantic-core-2.33.2 pydantic-evals-0.4.5 pydantic-graph-0.4.5 rsa-4.9.1 sse-starlette-2.4.1 tenacity-8.5.0 types-requests-2.32.4.20250611 urllib3-2.5.0 uvicorn-0.35.0 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8202d68",
   "metadata": {},
   "source": [
    "Let's import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7ff274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "# and create an agent\n",
    "chat_agent = Agent(  \n",
    "    'openai:gpt-4o-mini',\n",
    "    system_prompt=developer_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a57eca",
   "metadata": {},
   "source": [
    "Now we cam use it to automate tool description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746d3499",
   "metadata": {},
   "outputs": [
    {
     "ename": "UserError",
     "evalue": "Tool name conflicts with existing tool: 'search_tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUserError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[1;32m      4\u001b[0m \u001b[38;5;129;43m@chat_agent\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43msearch_tool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRunContext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43;03m    Search the FAQ for relevant entries matching the query.\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43;03m        and associated output IDs.\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquery\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages/pydantic_ai/agent.py:1552\u001b[0m, in \u001b[0;36mAgent.tool\u001b[0;34m(self, func, name, retries, prepare, docstring_format, require_parameter_descriptions, schema_generator, strict)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_toolset\u001b[38;5;241m.\u001b[39madd_function(\n\u001b[1;32m   1540\u001b[0m         func_,\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         strict,\n\u001b[1;32m   1549\u001b[0m     )\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func_\n\u001b[0;32m-> 1552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tool_decorator \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtool_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages/pydantic_ai/agent.py:1539\u001b[0m, in \u001b[0;36mAgent.tool.<locals>.tool_decorator\u001b[0;34m(func_)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtool_decorator\u001b[39m(\n\u001b[1;32m   1536\u001b[0m     func_: ToolFuncContext[AgentDepsT, ToolParams],\n\u001b[1;32m   1537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ToolFuncContext[AgentDepsT, ToolParams]:\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;66;03m# noinspection PyTypeChecker\u001b[39;00m\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_toolset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocstring_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequire_parameter_descriptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func_\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages/pydantic_ai/toolsets/function.py:196\u001b[0m, in \u001b[0;36mFunctionToolset.add_function\u001b[0;34m(self, func, takes_ctx, name, retries, prepare, docstring_format, require_parameter_descriptions, schema_generator, strict)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a function as a tool to the toolset.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mCan take a sync or async function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m        See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m tool \u001b[38;5;241m=\u001b[39m Tool[AgentDepsT](\n\u001b[1;32m    186\u001b[0m     func,\n\u001b[1;32m    187\u001b[0m     takes_ctx\u001b[38;5;241m=\u001b[39mtakes_ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[1;32m    195\u001b[0m )\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/llmenv/lib/python3.10/site-packages/pydantic_ai/toolsets/function.py:205\u001b[0m, in \u001b[0;36mFunctionToolset.add_tool\u001b[0;34m(self, tool)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a tool to the toolset.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    tool: The tool to add.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTool name conflicts with existing tool: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool\u001b[38;5;241m.\u001b[39mmax_retries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     tool\u001b[38;5;241m.\u001b[39mmax_retries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries\n",
      "\u001b[0;31mUserError\u001b[0m: Tool name conflicts with existing tool: 'search_tool'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@chat_agent.tool\n",
    "def search_tool(ctx: RunContext, query: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Search the FAQ for relevant entries matching the query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        The search query string provided by the user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of search results (up to 5), each containing relevance information \n",
    "        and associated output IDs.\n",
    "    \"\"\"\n",
    "    print(f\"search('{query}')\")\n",
    "    return search(query)\n",
    "\n",
    "\n",
    "@chat_agent.tool\n",
    "def add_entry_tool(ctx: RunContext, question: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Add a new question-answer entry to FAQ.\n",
    "\n",
    "    This function creates a document with the given question and answer, \n",
    "    tagging it as user-added content.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        The question text to be added to the index.\n",
    "\n",
    "    answer : str\n",
    "        The answer or explanation corresponding to the question.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    return add_entry(question, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca704e",
   "metadata": {},
   "source": [
    "It reads the functions' docstrings to automatically create function definition, so we don't need to worry about it.\n",
    "\n",
    "Let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"I just discovered the course. Can I join now?\"\n",
    "agent_run = await chat_agent.run(user_prompt)\n",
    "print(agent_run.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173466f9",
   "metadata": {},
   "source": [
    "If want to learn more about implementing chat applications with Pydantic AI:\n",
    "\n",
    "https://ai.pydantic.dev/message-history/\n",
    "https://ai.pydantic.dev/examples/chat-app/\n",
    "\n",
    "## Wrap up\n",
    "\n",
    "In this workshop, we took our RAG application and made it agentic, by first tweaking the prompts, and then using the \"function calling\" functionality from OpenAI.\n",
    "\n",
    "At the end, we put all the logic into the chat_assistant.py  script, and also explored PydanticAI to make it simpler.\n",
    "\n",
    "What's next:\n",
    "\n",
    "- MCP\n",
    "- Agent deployment\n",
    "- Agent monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
